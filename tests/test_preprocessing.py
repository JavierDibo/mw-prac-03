import unittest
import sys
import os
import pandas as pd # For pd.NaT and type checking
import numpy as np # For np.nan comparison if needed
import unittest.mock

# Add the parent directory (project root) to sys.path to allow imports from src
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from src.preprocessing import parse_log_line, COLUMN_NAMES, load_log_data, get_top_extensions

class TestPreprocessing(unittest.TestCase):
    # Path to the sample log file generated by create_test_sample.py
    SAMPLE_LOG_PATH = os.path.join(os.path.dirname(__file__), 'sample_first_2000_lines.txt')
    TEST_CSV_OUTPUT_PATH = os.path.join(os.path.dirname(__file__), 'test_top_extensions.csv')

    def tearDown(self):
        """Remove any files created during tests."""
        if os.path.exists(self.TEST_CSV_OUTPUT_PATH):
            os.remove(self.TEST_CSV_OUTPUT_PATH)
            # print(f"DEBUG: tearDown removed {self.TEST_CSV_OUTPUT_PATH}")

    def test_parse_valid_log_line_basic(self):
        line = '199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] "GET /history/apollo/ HTTP/1.0" 200 6245'
        expected_output = [
            '199.72.81.55', '-', '-', '01/Jul/1995:00:00:01 -0400',
            'GET', '/history/apollo/', 'HTTP/1.0', '200', '6245'
        ]
        self.assertEqual(parse_log_line(line), expected_output)

    def test_parse_valid_log_line_with_different_method(self):
        line = 'unicomp6.unicomp.net - - [01/Jul/1995:00:00:06 -0400] "POST /shuttle/countdown/ HTTP/1.1" 201 300'
        expected_output = [
            'unicomp6.unicomp.net', '-', '-', '01/Jul/1995:00:00:06 -0400',
            'POST', '/shuttle/countdown/', 'HTTP/1.1', '201', '300'
        ]
        self.assertEqual(parse_log_line(line), expected_output)

    def test_parse_log_line_hyphen_size(self):
        line = 'burger.letters.com - - [01/Jul/1995:00:00:11 -0400] "GET /shuttle/countdown/liftoff.html HTTP/1.0" 304 0' 
        expected_output = [
            'burger.letters.com', '-', '-', '01/Jul/1995:00:00:11 -0400',
            'GET', '/shuttle/countdown/liftoff.html', 'HTTP/1.0', '304', '0'
        ]
        self.assertEqual(parse_log_line(line), expected_output)
        
    def test_parse_log_line_truly_hyphen_size(self):
        line = 'another.host.com - - [01/Jul/1995:00:00:15 -0400] "GET /path HTTP/1.0" 200 -'
        expected_output = [
            'another.host.com', '-', '-', '01/Jul/1995:00:00:15 -0400',
            'GET', '/path', 'HTTP/1.0', '200', '-'
        ]
        self.assertEqual(parse_log_line(line), expected_output)

    def test_parse_invalid_log_line_incomplete(self):
        line = '199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] "GET /history/apollo/"' # Missing status and size
        self.assertIsNone(parse_log_line(line))

    def test_parse_log_line_malformed_date_is_extracted(self):
        # The regex captures any string within brackets for datetime.
        # Actual date validation will happen during pd.to_datetime conversion.
        line = '199.72.81.55 - - [01/Jul/1995/00:00:01 -0400] "GET / HTTP/1.0" 200 1234' # Slashes in date
        expected_output = [
            '199.72.81.55', '-', '-', '01/Jul/1995/00:00:01 -0400', # Malformed date is extracted as is
            'GET', '/', 'HTTP/1.0', '200', '1234'
        ]
        self.assertEqual(parse_log_line(line), expected_output)

    def test_parse_empty_line(self):
        line = ""
        self.assertIsNone(parse_log_line(line))

    def test_parse_line_with_only_spaces(self):
        line = "     "
        self.assertIsNone(parse_log_line(line))
        
    def test_column_names_reference(self):
        expected_column_names = [
            'Host remoto', 'Contraseña', 'Usuario', 'Fecha/Hora',
            'Método', 'Página', 'Protocolo', 'Resultado', 'Tamaño'
        ]
        self.assertEqual(COLUMN_NAMES, expected_column_names)

    def test_load_log_data_integration_with_larger_sample(self):
        self.assertTrue(os.path.exists(self.SAMPLE_LOG_PATH), 
                        f"Sample log file not found at {self.SAMPLE_LOG_PATH}. Please generate it first.")
        
        df = load_log_data(self.SAMPLE_LOG_PATH)
        self.assertIsNotNone(df, "DataFrame should not be None after loading sample.")

        # Check number of rows - should be around 2000, but can be less if some lines are unparseable by regex or completely blank
        # We expect most of the 2000 lines from the sample to be parseable.
        self.assertGreater(len(df), 1500, "DataFrame should have a significant number of rows (e.g., >1500 from 2000 lines sample).")
        self.assertLessEqual(len(df), 2000, "DataFrame should not have more than 2000 rows from 2000 lines sample.")
        
        # Check columns (ensure Fecha/Hora_UTC and marca de tiempo are present)
        # Assuming Fecha/Hora_UTC is still an intermediate column in your load_log_data
        expected_cols = COLUMN_NAMES + ['Fecha/Hora_UTC', 'marca de tiempo'] 
        self.assertEqual(list(df.columns), expected_cols, "DataFrame columns are incorrect.")

        # Check dtypes
        self.assertTrue(pd.api.types.is_datetime64_any_dtype(df['Fecha/Hora']), "'Fecha/Hora' should be datetime dtype")
        self.assertTrue(pd.api.types.is_datetime64_any_dtype(df['Fecha/Hora_UTC']), "'Fecha/Hora_UTC' should be datetime dtype")
        if not df['Fecha/Hora_UTC'].empty:
            self.assertTrue(df['Fecha/Hora_UTC'].dropna().dt.tz is not None, 
                            "Non-NaT 'Fecha/Hora_UTC' entries should be timezone-aware (UTC)")
        
        self.assertTrue(pd.api.types.is_numeric_dtype(df['Resultado']), "'Resultado' should be numeric")
        self.assertTrue(pd.api.types.is_float_dtype(df['Tamaño']), "'Tamaño' should be float dtype")
        self.assertTrue(pd.api.types.is_float_dtype(df['marca de tiempo']), "'marca de tiempo' should be float dtype")

        # Check for some expected NaN/NaT values based on real-world data patterns
        # It's highly probable that some 'Tamaño' entries were '-' and are now NaN
        if not df.empty:
            self.assertTrue(df['Tamaño'].isnull().any(), 
                            "Expected some NaN values in 'Tamaño' column from typical log data.")

        # It's also possible some dates couldn't be parsed, leading to NaT in Fecha/Hora
        # and consequently NaN in 'marca de tiempo'. This is not guaranteed but can be checked.
        # If this fails, it might just mean the first 2000 lines are perfectly clean date-wise.
        # print(f"DEBUG: NaT count in Fecha/Hora: {df['Fecha/Hora'].isnull().sum()}")
        # print(f"DEBUG: NaN count in marca de tiempo: {df['marca de tiempo'].isnull().sum()}")
        
        # Check that all 'marca de tiempo' are non-negative if their corresponding date is not NaT
        if not df.empty:
            valid_timestamps = df.loc[df['Fecha/Hora'].notna(), 'marca de tiempo']
            if not valid_timestamps.empty:
                self.assertTrue((valid_timestamps >= 0).all(), 
                                "All valid 'marca de tiempo' should be non-negative (>= 0 seconds from 1995-01-01 UTC).")

    def test_get_top_extensions_normal_case_and_csv_save(self):
        data = {
            'Página': [
                '/path/to/file.html', '/another/file.HTML', '/noext', '/path/doc.pdf',
                '/path/doc.pdf', '/archive.tar.gz', '/image.jpeg', '/image.jpeg',
                '/image.jpeg', '/script.js', '/data.xml', None, '/'
            ]
        }
        df_input = pd.DataFrame(data)
        
        # Test with top_n = 3
        expected_top_3 = pd.DataFrame({
            'Extensión': ['jpeg', 'pdf', 'html'], # Order by count, then alphabetically for ties
            'Número de Repeticiones': [3, 2, 2]
        })

        # Suppress print statements from the function during test
        with unittest.mock.patch('builtins.print') as mock_print:
            top_extensions_df = get_top_extensions(df_input, top_n=3, save_to_csv_path=self.TEST_CSV_OUTPUT_PATH)
        
        pd.testing.assert_frame_equal(top_extensions_df.sort_values(by=['Número de Repeticiones', 'Extensión'], ascending=[False, True]).reset_index(drop=True), 
                                      expected_top_3.sort_values(by=['Número de Repeticiones', 'Extensión'], ascending=[False, True]).reset_index(drop=True))

        # Verify CSV file creation and content
        self.assertTrue(os.path.exists(self.TEST_CSV_OUTPUT_PATH), "CSV file should have been created.")
        df_from_csv = pd.read_csv(self.TEST_CSV_OUTPUT_PATH)
        pd.testing.assert_frame_equal(df_from_csv.sort_values(by=['Número de Repeticiones', 'Extensión'], ascending=[False, True]).reset_index(drop=True),
                                      expected_top_3.sort_values(by=['Número de Repeticiones', 'Extensión'], ascending=[False, True]).reset_index(drop=True))

    def test_get_top_extensions_no_pagina_column(self):
        df_input = pd.DataFrame({'OtraColumna': [1, 2, 3]})
        with unittest.mock.patch('builtins.print') as mock_print:
            result_df = get_top_extensions(df_input)
        self.assertTrue(result_df.empty, "Should return an empty DataFrame if 'Página' column is missing.")
        mock_print.assert_any_call("Error: La columna 'Página' no existe en el DataFrame.")

    def test_get_top_extensions_no_valid_extensions(self):
        data = {'Página': ['/noext1', '/nodotext', None, '/', '']}
        df_input = pd.DataFrame(data)
        expected_df = pd.DataFrame({'Extensión': [], 'Número de Repeticiones': []}).astype({'Número de Repeticiones': 'object'})
        
        with unittest.mock.patch('builtins.print') as mock_print:
            result_df = get_top_extensions(df_input)
        
        pd.testing.assert_frame_equal(result_df, expected_df, check_dtype=False)
        mock_print.assert_any_call("No se encontraron extensiones válidas en la columna 'Página'.")

    def test_get_top_extensions_top_n_greater_than_found(self):
        data = {'Página': ['file.txt', 'another.txt', 'doc.pdf']}
        df_input = pd.DataFrame(data)
        # Expected: txt:2, pdf:1
        # We sort by count (desc) then extension (asc) for consistent comparison
        expected_data = {
            'Extensión': ['txt', 'pdf'],
            'Número de Repeticiones': [2, 1]
        }
        expected_df = pd.DataFrame(expected_data)
        
        with unittest.mock.patch('builtins.print') as mock_print:
            result_df = get_top_extensions(df_input, top_n=5) # top_n=5, but only 2 unique extensions
        
        # Sort both DataFrames for comparison
        result_df_sorted = result_df.sort_values(by=['Número de Repeticiones', 'Extensión'], ascending=[False, True]).reset_index(drop=True)
        expected_df_sorted = expected_df.sort_values(by=['Número de Repeticiones', 'Extensión'], ascending=[False, True]).reset_index(drop=True)
        
        pd.testing.assert_frame_equal(result_df_sorted, expected_df_sorted)

if __name__ == '__main__':
    unittest.main() 